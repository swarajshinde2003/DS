{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance with Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/swaraj.shinde/AppData/Local/Microsoft/WindowsApps/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# üöÄ Function to calculate MAPE (Mean Absolute Percentage Error)\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": Ridge(),\n",
    "    \"Lasso Regression\": Lasso(),\n",
    "    \"ElasticNet Regression\": ElasticNet(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "    \"Random Forest\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    \"AdaBoost Regressor\": AdaBoostRegressor(n_estimators=100, random_state=42),\n",
    "    \"Support Vector Regression\": SVR(kernel='rbf'),\n",
    "    \"K-Nearest Neighbors\": KNeighborsRegressor(n_neighbors=5),\n",
    "    \"XGBoost Regressor\": XGBRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Train and evaluate models\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train, y_train)  # Train the model\n",
    "    y_pred = model.predict(X_test)  # Predict on test set\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\"RMSE\": rmse, \"MAE\": mae, \"R¬≤\": r2, \"MAPE\": mape}\n",
    "    \n",
    "    # Scatter plot for actual vs. predicted values\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.scatterplot(x=y_test, y=y_pred, alpha=0.6, edgecolor='k')\n",
    "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='dashed')  # 45-degree line\n",
    "    plt.xlabel(\"Actual Score\")\n",
    "    plt.ylabel(\"Predicted Score\")\n",
    "    plt.title(f\"{name}: Actual vs. Predicted\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature Importance (Tree-Based Models)\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        feature_importance = model.feature_importances_\n",
    "    else:\n",
    "        # For other models, use permutation importance\n",
    "        perm_importance = permutation_importance(model, X_test, y_test, scoring=\"r2\", n_repeats=10, random_state=42)\n",
    "        feature_importance = perm_importance.importances_mean\n",
    "\n",
    "    # Feature Importance DataFrame\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'Feature': X_train.columns,\n",
    "        'Importance': feature_importance\n",
    "    }).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    # Plot Feature Importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=\"Importance\", y=\"Feature\", data=feature_importances[:15], palette=\"viridis\")  # Top 15 features\n",
    "    plt.title(f\"{name}: Top 15 Feature Importances\")\n",
    "    plt.xlabel(\"Importance Score\")\n",
    "    plt.ylabel(\"Feature Name\")\n",
    "    plt.show()\n",
    "\n",
    "# Convert results to DataFrame for comparison\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\nüìä Model Performance Comparison:\\n\")\n",
    "print(results_df)\n",
    "\n",
    "# Save the best model based on RMSE\n",
    "best_model_name = results_df[\"RMSE\"].idxmin()\n",
    "best_model = models[best_model_name]\n",
    "joblib.dump(best_model, \"best_regression_model.joblib\")\n",
    "print(f\"\\nüèÜ Best model: {best_model_name} saved as 'best_regression_model.joblib'\")\n",
    "#  .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
